{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Optimization\n",
    "## Loic Landrieu. Feb 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data and Library Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/ Load and plot the training data. Which model seems to have generated the data? In particular, how many \"centroids\" can you count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.genfromtxt('train.dat', delimiter=\" \", dtype='f8')\n",
    "data_test = np.genfromtxt('test.dat', delimiter=\" \", dtype='f8')\n",
    "n_point, n_dimension = data_train.shape\n",
    "n_state = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_train[:,0], data_train[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 First approach: k-means algorithm\n",
    "\n",
    "2/ Use the $k$-means algorithm in order to cluster the data into $K$ classes stored in vector $z\\in\\{0,1,2,3\\}^T$. Return the the vector $\\tau \\in \\{0,1\\}^{T \\times K}$ such that\n",
    "$\n",
    "\\tau_{t,k} = \n",
    "\\begin{cases}\n",
    "1 & \\text{if}\\;\\;z_t=k\\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Hint: use the $\\texttt{sklearn}$ library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(x, K):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    x : [T,D], T points of dimension D \n",
    "    K : integer, number of clusters\n",
    "    outputs:\n",
    "    tau [T,k] output, one-hot encoding of the clustering in k classes\n",
    "    \"\"\"\n",
    "    T = x.shape[0]\n",
    "    kmeans = KMeans(n_clusters=K, random_state=0).fit(x)\n",
    "    z = kmeans.labels_\n",
    "    tau = np.zeros((T, K),dtype='f8')\n",
    "    #FILL TAU\n",
    "    return tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3/ Compute and plot the k-means affectation for $K=4$. <font color=red> What assumption of the $k$-means algorithm is not well-suited to the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_affectation(x, cluster_indices):\n",
    "    colors = ['k', 'r', 'g', 'b']\n",
    "    markers = [ 'o', 's', '+', 'x']\n",
    "    for i_state in range(n_state):\n",
    "        plt.scatter(x[cluster_indices==i_state,0], x[cluster_indices==i_state,1], c=colors[i_state], marker=markers[i_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tau=#TODO\n",
    "plot_affectation(data_train, #TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 EM-algorithm for the Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first model the data by a Gaussian Mixture Model (GMM): the observed data is comprised of $T$ i.i.d random variables $X_t \\mapsto \\mathbb{R}^{2}$  which follow a mixture of $K$ Gaussians. The choice of the Gaussian component in the mixture is determined by a latent variable $Z_t \\mapsto \\{0,\\cdots,K-1\\}$ following a multinomial distribution.\n",
    "To each entry $t \\in [0,\\cdots,T-1]$ we associate an observation $x_t$ and a latent variable $z_t$, and denote by $x\\in \\mathbb{R}^{T \\times 2}$ the vector of $T$ observations and $z\\in \\{0,\\cdots,K-1\\}^{T}$ a corresponding vector of latent variables.\n",
    "\n",
    "$$\n",
    "p(x,z) = \\prod_{t=0}^{T-1} p(x_t,z_t) = \\prod_{t=0}^{T-1} p(z_t) p(x_t \\mid z_t)~.\n",
    "$$\n",
    "\n",
    "The latent variables follow each a multinomial distribution parameterized by $\\pi \\in \\mathbb{R}^K$: \n",
    "$$\n",
    "p(z_t=k) = \\pi_k~.\n",
    "$$\n",
    "The conditional probability $p(x_t \\mid z_t=k)$ follows a Gaussian distribution parameterized by the centroid $\\mu_k \\in \\mathbb{R}^{2}$ and covariance  matrix $\\Sigma_k \\in \\mathbb{R}^{ 2 \\times 2}$:\n",
    "$$\n",
    "p(x_t\\mid z_t=k) \\sim \\frac1{(2\\pi)^{D/2} \\mid\\Sigma_i\\mid^{1/2}}\n",
    "\\exp{\\left(-\\frac12(x_t-\\mu_k)^\\intercal \\Sigma_k^{-1} (x_t-\\mu_k)\\right)},\n",
    "$$\n",
    "\n",
    "5 / Draw the model in plate notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 / Complete the function $\\texttt{log_gaussian}(x, mu, Sigma)$ which returns the logarithm of the emission probability of value $x$ by a normal distribution parameterized by $\\mu$ and $\\Sigma$. Run the test cell to check if your code is correct.\n",
    "\n",
    "Hint: use `np.linalg.det` and `np.linalg.inv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gaussian(x, mu, Sigma):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    x : [T,D], input T points of dimension D\n",
    "    mu : [D,], centroid\n",
    "    Sigma : [D,D], centroid\n",
    "    outputs:\n",
    "    float, log probability of x being generated by the gaussian N(mu, sigma)\n",
    "    \"\"\"\n",
    "    vec_diff = x - mu\n",
    "    return #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the next assert checks the correcteness of the log-gaussian function's code! the assert must pass\n",
    "mu = np.array([1,1])\n",
    "Sigma = np.array([[1,0.5],[0.3,2]])\n",
    "x = np.array([[1,0],[10,1]])\n",
    "assert((np.abs(log_gaussian(x, mu, Sigma)-np.array([ -2.41574016, -45.92925367]))<1e-8).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 / Complete the function $\\texttt{log_all_gaussians}(x, mus, Sigmas)$ which returns $p(x_t \\mid z_t=k)$ for all the $k$ Gaussian distributions parameterized by $\\mu_k$ and $\\Sigma_k$:\n",
    "$$\n",
    "[p]_{t,k}=p(x_t \\mid z_t=k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_all_gaussians(x, mus, Sigmas):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    x : [T,D] T input points of dimension D\n",
    "    mus : [K,D] the K centroids\n",
    "    Sigmas : [K,D,D] the K covariances\n",
    "    outputs:\n",
    "    [T,K]  probability that each data point x_t was generated by the k-th mixture\n",
    "    \"\"\"\n",
    "    n_state = mus.shape[0]\n",
    "    log_proba = np.zeros((x.shape[0], n_state),dtype='f8')\n",
    "    for i_state in range(n_state):\n",
    "        log_proba[:,i_state] = #TODO\n",
    "    return log_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another code check\n",
    "mus = np.array([[1,1],[5,5]])\n",
    "Sigmas = np.array([[[1,0.5],[0.3,2]],[[2,0.5],[0.9,2]]])\n",
    "x = np.array([[1,0],[10,1]])\n",
    "assert((np.abs(log_all_gaussians(x, mus, Sigmas)-np.array([[ -2.41574016, -10.07698467], [-45.92925367, -17.96430861]]))<1e-7).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 / Complete the function $\\texttt{M_step_mixture}$ which implements the M-step of an EM algorithm for a GMM model. This function returns the parameters $\\hat{\\pi},\\hat{\\mu}, \\hat{\\Sigma}$ learnt from the expected latent variables $\\tau_{t,k}=p(z_t=k \\mid x; \\theta_\\text{current})$.\n",
    "\n",
    "Reminder:\n",
    "$$\n",
    "\\hat{\\pi}_k = \\frac{\\sum_{t=0}^{T-1}\\tau_{t,k}}{T} \n",
    "$$\n",
    "$$\n",
    "\\hat{\\mu}_k = \\frac{\\sum_{t=0}^{T-1}\\tau_{t,k} x_k}{\\sum_{t=0}^{T-1} \\tau_{t,k}} \n",
    "$$\n",
    "$$\n",
    "\\hat{\\Sigma}_k = \\frac{\\sum_{t=0}^{T-1}\\tau_{t,k} (x_t-\\hat{\\mu_k})(x_t-\\hat{\\mu_k})^\\intercal}{\\sum_{t=0}^{T-1} \\tau_{t,k}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#M-step\n",
    "def M_step_mixture(x, tau):\n",
    "    \"\"\"\n",
    "    x : [T,D] T input points of dimension D\n",
    "    tau : [T,K] expected latent variable\n",
    "    outputs:\n",
    "    pi : [K,] the Multinomial parameters of z\n",
    "    mus : [K,D] the K centroids\n",
    "    Sigmas : [K,D,D] the K covariances\n",
    "    \"\"\"\n",
    "    pi = np.zeros((n_state,),dtype='f8')\n",
    "    mus = np.zeros((n_state, n_dimension),dtype='f8')\n",
    "    Sigmas = np.zeros((n_state, n_dimension, n_dimension),dtype='f8')\n",
    "    tau_sum  = tau.sum(0)\n",
    "    pi = #TODO\n",
    "    mus = #TODO\n",
    "    for k_state in range(n_state):\n",
    "        x_diff = x - mus[k_state,:][None,:]\n",
    "        Sigmas[k_state,:,:] = #TODO\n",
    "    return pi, mus, Sigmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/ Compute $\\pi, (\\mu_k, \\Sigma_k)_k, $ with the value of $\\tau$ obtained with $k$-means. Use the function $\\texttt{plot_model}(\\tau, mus, Sigmas)$ to represent the iso-density contour of each Gaussians. <font color=red> Why does it make sens to use the $\\tau$ from k-means with an M-step to compute the parameters of the Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(x, tau, mus, Sigmas):\n",
    "    def plot_gaussian(mu, Sigma):\n",
    "        x, y = np.meshgrid(np.linspace(-10, 10, 100),\n",
    "                       np.linspace(-10, 10, 100))\n",
    "        def gauss(x,y): return np.exp(log_gaussian(np.array([x,y])[None,:], mu, Sigma)[0])\n",
    "        vec_gaussian = np.vectorize(gauss)\n",
    "        plt.contour(x, y, vec_gaussian(x, y),3)\n",
    "    plt.clf()\n",
    "    colors = ['k', 'r', 'g', 'b']\n",
    "    markers = [ 'o', 's', '+', 'x']\n",
    "    z = tau.argmax(1)\n",
    "    for k_state in range(n_state):\n",
    "        plt.scatter(x[z==k_state,0], x[z==k_state,1], c=colors[k_state], marker=markers[k_state])\n",
    "        plot_gaussian(mus[k_state,:], Sigmas[k_state,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau=kmeans(data_train, 4)\n",
    "pi, mus, Sigmas = M_step_mixture(data_train, tau)\n",
    "plot_model(data_train, tau, mus, Sigmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7/ Complete the function $\\texttt{logsumexp}(X)=\\log(\\sum_k(\\exp(X_k)))$ which compute the logarithm of sum on the last dimension of elementwise exponentials of an array in a robust way. Try it on the example below, and explain why the naive approach doesn't work.\n",
    "\n",
    "<font color=red> Justify why this might be necessary when handling normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    \"\"\"\n",
    "    #compute log(sum(exp(x))) avoiding numerical underflow\n",
    "    \"\"\"    \n",
    "    max_per_line = x.max(-1, keepdims=True)\n",
    "    return (max_per_line + np.log(np.sum(np.exp(x-max_per_line),-1, keepdims=True))).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_value = np.array([-1000, -2000, -1001])\n",
    "naive = np.log(np.sum(np.exp(test_value)))\n",
    "robust = logsumexp(test_value)\n",
    "print(\"naive = %f / robust = %f\" % (naive, robust))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 / Complete the function $\\texttt{E_step_mixture}(x, \\pi, mus, Sigmas)$ which returns the expected latent values $\\tau_{t,k}=p(z_t=k \\mid x: \\theta_\\text{current})$ given by the mixture parameterization $\\pi, (\\mu_k, \\Sigma_k)_k$. <font color='red'> Prove that <font color='black'>:\n",
    "$$\n",
    "p(z_t=k \\mid x; \\theta_\\text{current})=\\frac{p(x_t \\mid z_t=k; \\theta_\\text{current})\\;p(z_t=k; \\theta_\\text{current})}{\\sum_{k=0}^{K-1} p(x_t \\mid z_t=k; \\theta_\\text{current})\\;p(z_t=k; \\theta_\\text{current})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step_mixture(x, pi, mus, Sigmas):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    x : [T,D] T input points of dimension D\n",
    "    pi : [K,] the Multinomial parameters of z\n",
    "    mus : [K,D] the K centroids\n",
    "    Sigmas : [K,D,D] the K covariances\n",
    "    outputs:\n",
    "    tau : [T,K] expected latent variable\n",
    "    \"\"\"\n",
    "    log_tau_unnormalized = #TODO\n",
    "    return #TODO   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that the taus are probabilities\n",
    "tau = E_step_mixture(data_train, pi, mus, Sigmas)\n",
    "assert((np.abs(tau.sum(1)-1)<1e-8).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 / Complete the function $\\texttt{avg_log_likelihood_mixture}(x)=\\log(p(x;\\theta_\\text{current}))$ which returns the loglikelihood of the vector of observations  $x$ given a Gaussian mixture model parameterized by $\\pi, (\\mu_k, \\Sigma_k)_k$ and normalized by the number of observations.\n",
    "$$\n",
    "L(x;\\theta_\\text{current})=\\frac1T \\sum_{t=0}^{T-1}\\log(p(x_t;\\theta_\\text{current}))=\\frac1T \\sum_{t=0}^{T-1}\\log\\left(\\sum_{k=0}^{K-1} p(x_t,z_t=k;\\theta_\\text{current})\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_log_likelihood_mixture(x, pi, mus, Sigmas):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    x : [T,D] T input points of dimension D\n",
    "    pi : [K,] the Multinomial parameters of z\n",
    "    mus : [K,D] the K centroids\n",
    "    Sigmas : [K,D,D] the K covariances\n",
    "    outputs:\n",
    "    log-likelihood of x\n",
    "    \"\"\"\n",
    "    log_tau_unnormalized = log_all_gaussians(x, mus, Sigmas) + np.log(pi[None,:])#actually did it for you, nothing to complete\n",
    "    return (logsumexp(log_tau_unnormalized).sum()) / x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 / Implement the EM algorithm to learn the mixture parameters $\\pi, (\\mu_k, \\Sigma_k)_k$ from the training set with $K=4$. Initialize with $k$-means. Print the likelihood at each step, what do we observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_mixture(x):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    x : [T,D] T input points of dimension D\n",
    "    outputs:\n",
    "    pi0 : [K,] initial Multinomial\n",
    "    mus : [K,D] the K centroids\n",
    "    Sigmas : [K,D,D] the K covariances\n",
    "    tau : [T,K] expected latent variable\n",
    "    \"\"\"\n",
    "    n_ite = 50\n",
    "    #first E-step\n",
    "    tau = kmeans(x, 4)\n",
    "    #main loop\n",
    "    for i_ite in range(n_ite):\n",
    "        #M-step\n",
    "        pi, mus, Sigmas = #TODO\n",
    "        #E-step\n",
    "        tau = #TODO\n",
    "        #likelihood\n",
    "        print(\"Mixture iteration %d  - loglikelihood = %1.3f\" % (i_ite, #TODO))\n",
    "    return pi, mus, Sigmas, tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi, mus, Sigmas, tau = EM_mixture(data_train)\n",
    "plot_model(data_train, tau, mus, Sigmas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train loglikelihood = %f\" % avg_log_likelihood_mixture(data_train, pi, mus, Sigmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 / Compute the log-likelihood of the test data, visualize the data and comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = E_step_mixture(data_test, pi, mus, Sigmas)\n",
    "plot_model(data_test, tau, mus, Sigmas)\n",
    "print(\"Test loglikelihood = %f\" % avg_log_likelihood_mixture(data_test, pi, mus, Sigmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Hidden Markov Model\n",
    "\n",
    "10 / We now take into account the temporal structure of the data with a chain-structured hidden Markov model with discrete latent variable $z\\in\\{0,\\cdots,K-1\\}^T$:\n",
    "$$\n",
    "p(x, z) = p(z_0)\\prod_{t=0}^{T-1}  p(x_t \\mid z_t)\\prod_{t=1}^{T-1} p(z_t \\mid z_{t-1}).\n",
    "$$\n",
    "We model $p(x_t \\mid z_t = k) \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)$ like in the previous model, and $p(z_{t+1}=k \\mid z_{t}=l)=A_{k,l}$ and finally $p(z_0)$ with a multinomial distribution parameterized by $\\pi_0$.\n",
    "\n",
    "We initialize the Gaussian parameters $\\mu_k$ and $\\Sigma_k$ with the values $\\hat{\\mu_k}$ and $\\hat{\\Sigma_k}$ computed in the mixture model. We initalize $\\pi_0$ and $A$ as follows:\n",
    "$$\n",
    "\\pi_0=\\hat{\\pi}\n",
    "$$\n",
    "\n",
    "$$\n",
    "[A]_{k,l}=\\hat{\\pi}_k\n",
    "$$\n",
    "Complete the next cell to initialize $A$ and $\\pi_0$. <font color = red> How can we interpret this initialization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = #TODO\n",
    "pi0 = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 / Complete the function $\\texttt{alpha_beta}(x, pi,mus,Sigmas, A)$ which peforms the marginal inference of the model.\n",
    "\n",
    "we have the following formulas for the alpha and beta recursions:\n",
    "$$\n",
    "\\alpha(z_{t}) = p(x_0, \\cdots, x_t, z_t)  = p(x_t \\mid z_t)\\sum_{z_{t-1}=0}^{K-1}\\alpha(z_{t-1})\\;p(z_{t} \\mid z_{t-1})\n",
    "$$\n",
    "$$\n",
    "\\beta(z_{t}) = p(x_{t+1}, \\cdots, x_T \\mid z_t) = \\sum_{z_{t+1}=0}^{K-1}\\beta(z_{t+1})\\;p(z_{t+1}\\mid z_{t})\\;p(x_t \\mid z_{t})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_beta(x, pi0, mus, Sigmas, A):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    x : [T,D] T input points of dimension D\n",
    "    pi0 : [K,] initial Multinomial\n",
    "    mus : [K,D] the K centroids\n",
    "    Sigmas : [K,D,D] the K covariances\n",
    "    A : [K,K] transition probability\n",
    "    outputs:\n",
    "    log_proba_emission : [T,K] the probability that each data point x_t was generated by the k-th mixture\n",
    "    alpha, beta : [T,K] \n",
    "    \"\"\"\n",
    "    T = x.shape[0]\n",
    "    log_proba_emission = log_all_gaussians(x, mus, Sigmas)\n",
    "\n",
    "    #forward pass\n",
    "    log_alpha = np.zeros((T, n_state), dtype=('f8'))\n",
    "    log_alpha[0,:] = #TODO\n",
    "    for t in range(1,T):\n",
    "        log_alpha[t,] = #TODO\n",
    "    #backward pass\n",
    "    log_beta = np.zeros((T, n_state), dtype=('f8'))  \n",
    "    for t in range(T-2,-1,-1):\n",
    "        log_beta[t,] = #TODO\n",
    "    return log_proba_emission, log_alpha, log_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_proba_emission, log_alpha, log_beta = alpha_beta(data_train, pi0, mus, Sigmas, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11 / Compute the first order marginal probability $\\gamma_t= {p(z_t \\mid x)}$. Check that the normalization factor is the same for all $t$. Why is that the case? \n",
    "Reminder:\n",
    "$$\n",
    "\\gamma(z_t)=p(z_t \\mid x) \\propto {\\alpha(z_t)\\beta(z_t)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#marginal inference - first order\n",
    "log_gamma_unnormalized = #TODO\n",
    "normalizing_factor = #TODO\n",
    "log_gamma = #TODO\n",
    "#check that the normalizing factor is indeed constant\n",
    "assert((np.abs(normalizing_factor.std())<1e-8).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the evolution of the most likely state for each of the first 100 point and comment. <font color = red> How good was our initialization of $A$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = #TODO\n",
    "figure(figsize=(12,4))\n",
    "plt.plot(z[0:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12 / Compute the pairwise marginals $\\xi(z_{t-1},z_t)=p(z_{t-1},z_{t} \\mid x; \\theta_\\text{current})$.\n",
    "\n",
    "Reminder:\n",
    "$$\n",
    "\\xi(z_{t-1},z_t)= \\frac{\\alpha(z_{t-1}) p(z_t \\mid X; \\theta_\\text{current}) p(z_t \\mid z_{t-1}; \\theta_\\text{current}) \\beta(z_t)}{p(x; \\theta_\\text{current})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_xi = #TODO\n",
    "#code check: the pair marginals are indeed probabilities\n",
    "assert((np.abs(np.exp(log_xi).sum(1).sum(1)-1)<1e-10).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13 / Complete the function $\\texttt{E_step_HMM}(pi,mus,Sigmas, A)$ which returns the singleton and pairwise marginals as well as the averaged loglikelihood of the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step_HMM(x, pi0, mus,Sigmas, A):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    x : [T,D] T input points of dimension D\n",
    "    pi0 : [K,] initial Multinomial\n",
    "    mus : [K,D] the K centroids\n",
    "    Sigmas : [K,D,D] the K covariances\n",
    "    A : [K,K] transition probability\n",
    "    outputs:\n",
    "    log_gamma : [T,K] marginals\n",
    "    log_xi : [T, K, K] patwise marginals\n",
    "    avg likelihood, float\n",
    "    \"\"\"\n",
    "    #\n",
    "    log_proba_emission, log_alpha, log_beta = #TODO\n",
    "    log_gamma_unnormalized = #TODO\n",
    "\n",
    "    log_gamma = #TODO\n",
    "    log_xi = #TODO\n",
    "    return log_gamma, log_xi, log_likelihood[0]/x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14 / Complete the function $\\texttt{M_step_HMM}(\\gamma, \\xi)$ for the HMM model which returns the parameters $\\hat{\\mu}_k,\\hat{\\Sigma}_k, \\hat{A}$.\n",
    "\n",
    "$$\n",
    "[\\hat{A}]_{k,l}=\\frac{\\sum_{t=0}^{T-2}p(z_{t+1}=k,z_{t}=l \\mid x; \\theta_\\text{current})}{\\sum_{t=0}^{T-2}\\sum_{j=0}^{K-1} p(z_{t+1}=j,z_{t}=l \\mid x; \\theta_\\text{current})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step_HMM(x, log_gamma, log_xi):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    x: [T,D] T input points of dimension D\n",
    "    log_gamma [T,K] marginals\n",
    "    log_xi [T, K, K] patwise marginals\n",
    "    outputs:\n",
    "    pi0 : [K,] initial Multinomial\n",
    "    mus : [K,D] the K centroids\n",
    "    Sigmas : [K,D,D] the K covariances\n",
    "    A : [K,K] transition probability\n",
    "    \"\"\"\n",
    "    pi, mus, Sigmas = #TODO\n",
    "    A = #TODO\n",
    "    pi0 = np.exp(log_gamma[0,:])\n",
    "    return pi0, mus, Sigmas, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi0, mus, Sigmas, A =  M_step_HMM(data_train, log_gamma, log_xi)\n",
    "#a code check that A is indeed a conditional probability\n",
    "assert(np.all(np.abs(A.sum(0)-1)<1e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16 / Implement the EM algorithm for the HMM model. Initialize the parameters with EM on the Gaussian mixture model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init with mixture\n",
    "pi, mus, Sigmas, tau = EM_mixture(data_train)\n",
    "A = np.tile(pi, (n_state,1))\n",
    "pi0 = pi\n",
    "n_ite = 100\n",
    "#main loop\n",
    "for i_ite in range(100):    \n",
    "    #E-step\n",
    "    log_gamma, log_xi, log_likelihood = #TODO\n",
    "    print(\"HMM iteration %d  - likelihood = %1.3f\" % (i_ite, log_likelihood))\n",
    "    #M-step\n",
    "    pi0, mus, Sigmas, A = #TODO\n",
    "    \n",
    "plot_model(data_train, np.exp(log_gamma), mus, Sigmas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18/ Compute the final loglikelihood on the training set and compare it to the one obtained with the mixture model. <font color='red'> Is it a surprising result? Explain why? Make the same comparison on the test set. What conclusion can we draw now?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_log_gamma, _log_xi, log_likelihood_train = E_step_HMM(data_train, pi0, mus, Sigmas, A)\n",
    "_log_gamma, _log_xi, log_likelihood_test = E_step_HMM(data_test, pi0, mus, Sigmas, A)\n",
    "print(\"likelihood train %1.3f  - likelihood test= %1.3f\" % (log_likelihood_train, log_likelihood_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19/ <font color = red> If we didn't know in advance the number of states, how would we choose it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20 / Implement the Viterbi algorithm for MAP inference / decoding. <font color = red> Explain the difference between marginal and MAP inference.\n",
    "</font>\n",
    "\n",
    "Reminder:\n",
    "the Viterbi algorithm is also called max-product. It computes the sequence of state with the maximum likelihood. It defines $p^\\text{map}_{t,i}$ as the probability that the most probable path $0\\cdots,t$ end in state $i$. We have the following recursion:\n",
    "$$\n",
    "p^\\text{map}_{t,i} = p(x_t,z_t=i;\\theta_\\text{current}) \\;\\max_j( p^\\text{map}_{t-1,j}\\; p(z_t=i \\mid z_{t-1}=j;\\theta_\\text{current})\n",
    "$$\n",
    "Once $p^\\text{map}_{T,i}$ is computed, the last state of the map sequence is $\\text{argmax}(p^\\text{map}_{T,i})$. To compute the previous state, one simply backtrack through the selected transition computed earlier (keep track of the argmax!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(x, pi0, mus, Sigmas, A):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    pi0 : [K,] initial Multinomial\n",
    "    mus : [K,D] the K centroids\n",
    "    Sigmas : [K,D,D] the K covariances\n",
    "    A : [K,K] transition probability\n",
    "    outputs:\n",
    "    map_state T, the MAP affectation\n",
    "    best_previous_state : the argmax of each max-product\n",
    "    \"\"\"\n",
    "    T = x.shape[0]\n",
    "    log_proba_emission = #TODO\n",
    "    map_state = np.zeros((T,), dtype='uint8') #the map sequence\n",
    "    best_proba = np.zeros((T,n_state), dtype='f8') #probability of best sequence 1..t being in state i\n",
    "    best_previous_state = np.zeros((T,n_state), dtype='uint8') #state of the previous observation in the best sequence 1..t\n",
    "    #init\n",
    "    best_proba[0,:] = #TODO\n",
    "    best_previous_state[0,:] = np.NaN #no previous observation\n",
    "    #forward pass\n",
    "    for t in range(1,T):\n",
    "        proba_previous_state = #TODO\n",
    "        best_proba[t,] = #TODO\n",
    "        best_previous_state[t,] = np.argmax(proba_previous_state,1)\n",
    "    #backward pass\n",
    "    map_state[-1] = best_proba[-1,:].argmax()\n",
    "    for t in range(T-2,0,-1):#backtrack\n",
    "        map_state[t] = #TODO\n",
    "    return map_state, best_previous_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_state, best_previous_state = viterbi(data_train, pi0, mus,Sigmas, A)\n",
    "plot_affectation(data_train, map_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21 / Launch the visualiziation below representing the backtracking of the Viterbi algorithm. Interpret and comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(num=None, figsize=(8, 100), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.axis('off')\n",
    "for t in range(n_point-2,n_point-100,-1):\n",
    "    for i_state in range(n_state):\n",
    "        if best_previous_state[t,i_state]==map_state[t-1] and i_state==map_state[t]:\n",
    "            c = 'r'\n",
    "        else:\n",
    "            c = 'k'\n",
    "        plt.plot([i_state, best_previous_state[t,i_state]],[t, t-1], c)\n",
    "best_previous_state[-10:-1,:] \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
