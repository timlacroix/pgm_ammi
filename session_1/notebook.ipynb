{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram Mixture model\n",
    "\n",
    "\n",
    "##### The model :\n",
    "Recall the unigram mixture model in plate notations :\n",
    "<img src=\"img/unigram_mixture_new.png\" alt=\"unigram mixture\" width=\"200\"/>\n",
    "\n",
    "To sample from this model :\n",
    "\n",
    "First, we have to sample the topic of a document as a one-hot vector, from a multinomial of size $K$.\n",
    "- $z \\sim \\mathcal{M}(1, (\\pi_1, \\dots, \\pi_K)), z\\in\\{0, 1\\}^K$\n",
    "- $p(z)=\\prod_{k=1}^K\\pi_k^{z_k}$\n",
    "\n",
    "Once the topic $z^{(i)}$ is selected for document $i$, we can the $N$ words of each documents from a multinomial of size $d$, which is the vocabulary size.\n",
    "- $w^{(i)}_n~|~\\{z^{(i)}_k = 1\\} \\sim \\mathcal{M}(1, (b_{1k},\\dots,b_{dk}))$\n",
    "\n",
    "\n",
    "\n",
    "Finally :\n",
    "$\\displaystyle{p(w^{(i)}, z^{(i)}) = \\prod_{k=1}^K \\Big[\\pi_k^{z_k}\\prod_{j=1}^d \\prod_{n=1}^{N}b_{jk}^{w^{(i)}_{n,j}z_k}\\Big]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 0:\n",
    "Let $x^{(i)}_j=\\sum_{n=1}^N w_{nj}^{(i)}$. What is the distribution of $x^{(i)}~|~\\{z^{(i)}_k = 1\\}$ ?\n",
    "\n",
    "In the rest of the session, we will work with word counts $x^{(i)}$ as done in the slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "Sample $M=400$ documents of $N=30$ words from the given $\\pi$ and $b$ in the next cell. We have $d=3$ words, but $K=6$ topics.\n",
    "Complete the function _gen_\\__corpus_ below that takes all the required parameters as input, and returns a $(M,d)$ array containg the $x^{(i)}$, as well as an $(M,)$ array containing the topics associated with these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_docs = 400\n",
    "doc_length = 30\n",
    "n_topics = 6\n",
    "n_tokens = 3 # size of the dictionary\n",
    "\n",
    "topic_proba = np.array([1./n_topics] * n_topics)\n",
    "word_proba = np.array([\n",
    "    [0.8, 0.1, 0.1],\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.1, 0.1, 0.8],\n",
    "    [0.45, 0.45, 0.1],\n",
    "    [0.1, 0.45, 0.45],\n",
    "    [0.45, 0.1, 0.45]\n",
    "])\n",
    "\n",
    "\n",
    "# WARNING: word_proba and corpus are transposed versions of B and X seen in class.\n",
    "def gen_corpus(n_docs, n_topics, n_tokens, doc_length, topic_proba, word_proba):\n",
    "    \"\"\"\n",
    "    n_docs: number of documents, S\n",
    "    n_topics: number of topics, K\n",
    "    doc_length: number of words per documents, N\n",
    "    topic_proba: (K,) array containing the pi_k\n",
    "    word_proba: (K, d) array containing the b_{k,j}.\n",
    "    \n",
    "    returns: \n",
    "    corpus: (M, K) array containing the word counts for each documents\n",
    "    topics: (M, K) array containing one-hot vectors for the topic of each document\n",
    "    \"\"\"\n",
    "    corpus = np.empty((n_docs, n_tokens))\n",
    "    topics = # TODO: sample the topic of each documents\n",
    "\n",
    "    # For each document, sample the word counts x corresponding to the sampled topic\n",
    "    for doc_id, topic in enumerate(topics):\n",
    "        # topic is a one-hot vector, use `where` to find where the 1 is\n",
    "        topic_id = np.where(topic)[0][0]\n",
    "        pass  # TODO: Sample the count of words according to topic_id.\n",
    "    return corpus, topics\n",
    "\n",
    "corpus, topics = gen_corpus(n_docs, n_topics, n_tokens, doc_length, topic_proba, word_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data :\n",
    "In the next cell, we propose a visualization of this corpus. We will use the three vertices of a triangle to represent our three tokens. A document will be represented as a convex combination of the three vertices. We will color each document according to their topic (red, green, blue and black).\n",
    "Finally, each row in $b$ giving the token probability given the topic will also be visualized.\n",
    "\n",
    "### Question 2:\n",
    "Run the next cell to visualize the dataset. Explain what is shown in the obtained figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "# Compute the triangle vertices using complex roots of 1\n",
    "vertex_ids = np.array(range(n_tokens))\n",
    "vertices = np.vstack([\n",
    "    np.cos(2 * math.pi * vertex_ids / n_tokens),\n",
    "    np.sin(2 * math.pi * vertex_ids / n_tokens)]\n",
    ").T\n",
    "\n",
    "# Plot the triangle\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "polygon = Polygon(vertices, fill=False, linewidth=2, linestyle='--')\n",
    "ax.add_patch(polygon)\n",
    "ax.set_xlim([-1.1, 1.1])\n",
    "ax.set_ylim([-1.1, 1.1])\n",
    "\n",
    "# Compute a S x 2 array containing documents as linear combination of our vertices\n",
    "\n",
    "linear_combinations = (corpus / np.sum(corpus, axis=1)[:, None]) @ vertices  \n",
    "colors = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 0, 0], [1, 1, 0], [0, 1, 1]])\n",
    "\n",
    "plt.scatter(\n",
    "    linear_combinations[:, 0], linear_combinations[:, 1], c=np.array(topics) @ colors,\n",
    "    marker='o', s=500, alpha=0.6\n",
    ")\n",
    "\n",
    "centers = word_proba @ vertices\n",
    "\n",
    "plt.scatter(\n",
    "    centers[:, 0], centers[:, 1], c=colors,\n",
    "    marker='X', s=500, alpha=1, edgecolor=(0, 0, 0, 1), linewidths=2\n",
    ")\n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe documents $(x^{(i)})_{i=1..M}$, and apply EM for $t=1\\dots T$.\n",
    "\n",
    "\n",
    "##### E-Step:\n",
    "- $\\displaystyle{p(z^{(i)}_k=1~|~x^{(i)};b^{(t-1)};\\pi^{(t-1)}) = \\frac{\\pi^{(t-1)}_k \\prod_{j=1}^d(b_{jk}^{(t-1)})^{x_j}}{\\sum_{k'=1}^K\\pi^{(t-1)}_{k'}\\prod_{j=1}^d(b_{jk'}^{(t-1)})^{x_j}}}$\n",
    "- $q^{(t)}_{ik} = \\mathbb{E}[z_k~|~x^{(i)};b^{(t)};\\pi^{(t)}]$\n",
    "\n",
    "\n",
    "##### M-Step:\n",
    "- $\\displaystyle{b_{jk}^{(t)} = \\frac{\\sum_{i}x_j^{(i)}q_{ik}^{(t)}}{\\sum_{i,j'}x_{j'}^{(i)}q_{ik}^{(t)}}}$ and $\\displaystyle{\\pi_{k}^{(t)} = \\frac{\\sum_{i}q_{ik}^{(t)}}{\\sum_{i,k'}q_{ik'}^{(t)}}}$\n",
    "\n",
    "\n",
    "### Question 3:\n",
    "1. Re-derive the above formula for $b^{(t)}_{jk}$ starting from the expected complete log-likelihood :\n",
    "$$\\mathbb{E}_{q_i^{(t)}}[\\log(p(X, Z;b,\\pi)]=\\sum_{i,j,k}x_j^{(i)}q_{ik}^{(t)}\\log(b_{jk}) + \\sum_{i,k}q_{ik}^{(t)}\\log(\\pi_k) + \\tilde{c}$$\n",
    "2. It could happen in the computation of $q_{ik}^{(t)}$ that both numerator and denominator are very small. To avoid any numerical underflows, we will compute $\\log(q_{ik}^{(t)})$ from the logarithms of both numerator and denominator. We use the function logsumexp defined below to compute the denominator from the logarithms of the terms in the sum. Explain why we factored out the maximal value on each row ? \n",
    "3. Write code for logsumexp which computes $\\log(\\sum_i\\exp(x_i))$, beware of (2.). Write a test that shows the implementation works as expected. How can we use this function in the E step ?\n",
    "4. Fill in the code for the E and M steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    \"\"\"\n",
    "    x: a (n, m) array.\n",
    "    return: log(sum(exp(x_i,:))) for all i, avoiding numerical underflow\n",
    "    \"\"\"\n",
    "    # select the maximum on each row\n",
    "    max_per_line = x.max(-1)\n",
    "    res = max_per_line + np.log(np.sum(np.exp(x-max_per_line[:,None]),-1))\n",
    "    return res\n",
    "\n",
    "# test that our function does what it should on simple inputs\n",
    "x = np.array([[1, 2, 3], [2, 2, 2]]) \n",
    "print(logsumexp(x), np.log(np.sum(np.exp(x), axis=1)))\n",
    "\n",
    "# write a test case in which the basic method overflows and yours doesn't\n",
    "x = np.array([[...], [...]]) \n",
    "print(logsumexp(x), np.log(np.sum(np.exp(x), axis=1))) # \n",
    "\n",
    "\n",
    "def E_step(corpus, learned_topic_proba, learned_word_proba):\n",
    "    \"\"\"\n",
    "    corpus : (S, n_tokens) array containing the counts of words in each document\n",
    "    learned_topic_proba : (n_topics, ) array containing the probability of each topic\n",
    "    learned_topic_proba : (n_topics, n_token) array containing the probability of each word for each topic\n",
    "    returns : doc_topic_proba : (S, n_topicdlhkfiidgninkhvulrjvurlvhrdekcur) the q_ik\n",
    "    \"\"\"\n",
    "    # TODO: complete\n",
    "    return np.exp(log_qik)\n",
    "\n",
    "def M_step(corpus, doc_topic_probas):\n",
    "    \"\"\"\n",
    "    corpus : (S, n_tokens) array containing the counts of words in each document\n",
    "    doc_topic_probas : (S, n_topics) array containing q_ik computed in the E-step\n",
    "    \"\"\"\n",
    "    pass  # TODO: complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-Likelihood\n",
    "It is convenient to visualize the evolution of the marginal log-likelihood of our parameters during training to make sure it is being maximized.\n",
    "\n",
    "### Question 4:\n",
    "1. Prove mathematically that the marginal log-likelihood of the parameters during EM increases at each iteration.\n",
    "2. Implement the computation of the log-likelihood in the next cell. `log_likelihood_qik` will be used later in the session. Hint : `log_multinomial(corpus)` computes $\\tilde{c}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gammaln\n",
    "def log_multinomial(params):\n",
    "    return gammaln(np.sum(params, axis=1) + 1) - np.sum(gammaln(params + 1), axis=1)\n",
    "\n",
    "def log_likelihood_q_ik(corpus, doc_topic_probas, learned_word_proba, learned_topic_proba):\n",
    "    \"\"\"\n",
    "    corpus : (S, n_tokens) array containing the counts of words in each document\n",
    "    doc_topic_probas : (S, n_topics) array containing E_q[z] computed in the E-step\n",
    "    learned_topic_proba : (n_topics, ) array containing the probability of each topic\n",
    "    learned_word_proba : (n_topics, n_token) array containing the probability of each word for each topic\n",
    "    \"\"\"\n",
    "    first_term = corpus @ np.log(learned_word_proba.T)\n",
    "    first_term[np.isnan(first_term)] = 0\n",
    "    first_term *= doc_topic_probas\n",
    "    \n",
    "    second_term = doc_topic_probas @ np.log(learned_topic_proba)[:, None]\n",
    "    entropy = np.log(doc_topic_probas) * doc_topic_probas\n",
    "    entropy[np.isnan(entropy)] = 0\n",
    "    return (np.sum(first_term) + np.sum(second_term) - np.sum(entropy) + np.sum(log_multinomial(corpus))) / corpus.shape[0]\n",
    "\n",
    "def log_likelihood(corpus, learned_word_proba, learned_topic_proba):\n",
    "    \"\"\"\n",
    "    corpus : (S, n_tokens) array containing the counts of words in each document\n",
    "    learned_topic_proba : (n_topics, ) array containing the probability of each topic\n",
    "    learned_word_proba : (n_topics, n_token) array containing the probability of each word for each topic\n",
    "    \"\"\"\n",
    "    pass # TODO: complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running EM :\n",
    "We can now run the Expectation-Maximization algorithm on our corpus.\n",
    "We will initialize the algorithm by setting $\\pi_k = 1/K$.\n",
    "For $b_{k}$, we use the proportions of random documents.\n",
    "Then we run the algorithm for a few iterations.\n",
    "\n",
    "Upon completion, we plot the log likelihood curve for training and validation data.\n",
    "\n",
    "### Question 5:\n",
    "- Fill the TODOs in the code, and comment the results when asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_corpus, valid_topics = gen_corpus(1000, n_topics, n_tokens, doc_length, topic_proba, word_proba)\n",
    "\n",
    "n_iter = 20\n",
    "n_learned_topics = 6\n",
    "\n",
    "# We nitialize the learned parameter\n",
    "# uniform topic proba\n",
    "learned_topic_proba = np.ones((n_learned_topics,)) / n_learned_topics\n",
    "\n",
    "# word proba taken from random documents + some constant to avoid zeroes\n",
    "learned_word_proba = corpus[np.random.permutation(n_docs)[:n_learned_topics]] + 1e-3\n",
    "learned_word_proba = learned_word_proba / np.sum(learned_word_proba, axis=1)[:, None]\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "train_ll = []\n",
    "train_ll_after_E = []\n",
    "train_ll_E_step = []\n",
    "valid_ll = []\n",
    "for i in range(n_iter):\n",
    "    # TODO: step of the EM algorithm\n",
    "    # TODO: After E-step call log_likelihood_q_ik on the training data and append this value to train_ll_E_step\n",
    "    # TODO: After E-step, also call log_likelihood on the training data and  append this value to train_ll_after_E\n",
    "    # TODO: After M-step compute the log-likelihoods and append these values to train_ll and valid_ll\n",
    "    \n",
    "    \n",
    "    # This assert checks that each multinomial for learned_word_proba sums to 1\n",
    "    assert np.all(np.abs(np.sum(learned_word_proba, axis=1) - 1) < 1e-10)\n",
    "    # This assert checks that learned_topic_proba sums to 1\n",
    "    assert np.all(np.abs(np.sum(learned_topic_proba) - 1) < 1e-10)\n",
    "# compute valid log-likelihood for the true parameters of the model\n",
    "# TODO: plot train_ll, valid_ll, and valid log-likelihood for true parameters. comment\n",
    "# TODO: plot train_ll, train_ll_E_step, train_ll_after_E. comment\n",
    "\n",
    "\n",
    "    \n",
    "plt.xlabel(\"EM-iteration\")\n",
    "plt.ylabel(\"Data log-likelihood\")\n",
    "plt.legend(bbox_to_anchor=(1,1), loc=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6:\n",
    "Plot the learned word_proba along the dataset and original word_probas. Rerun the code with several different random initializations, comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the triangle\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "polygon = Polygon(vertices, fill=False, linewidth=2, linestyle='--')\n",
    "ax.add_patch(polygon)\n",
    "ax.set_xlim([-1.1, 1.1])\n",
    "ax.set_ylim([-1.1, 1.1])\n",
    "\n",
    "# Compute a S x 2 array containing documents as linear combination of our vertices\n",
    "linear_combinations = (corpus / np.sum(corpus, axis=1)[:, None]) @ vertices  \n",
    "\n",
    "plt.scatter(\n",
    "    linear_combinations[:, 0], linear_combinations[:, 1], c=np.array(topics) @ colors,\n",
    "    marker='o', s=500, alpha=0.2\n",
    ")\n",
    "\n",
    "# plot real word_proba\n",
    "\n",
    "centers = word_proba @ vertices\n",
    "\n",
    "plt.scatter(\n",
    "    centers[:, 0], centers[:, 1], c=colors,\n",
    "    marker='X', s=500, alpha=1, edgecolor=(0, 0, 0, 1), linewidths=2\n",
    ")\n",
    "\n",
    "# TODO:  plot the learned word_proba.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Larger synthetic dataset\n",
    "\n",
    "Now, we will generate a more realistic synthetic dataset. We will have $1000$ documents of $200$ words each, $10$ topics and $50$ different tokens.\n",
    "We will randomly generate the parameters of our generative model with uniform <em>topic_proba</em> and <em>word_proba</em> coming from a [Dirichlet](https://en.wikipedia.org/wiki/Dirichlet_distribution) distribution with all parameters equal to $\\alpha=0.5$.\n",
    "\n",
    "### Question 6:\n",
    "- How can we determine the right number of topics to model this data ?\n",
    "- Implement this idea in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_docs = 100\n",
    "n_valid_docs = 1000\n",
    "doc_length = 200\n",
    "n_topics = 10\n",
    "n_tokens = 50\n",
    "alpha = 0.5\n",
    "\n",
    "\n",
    "def gen_bigger_corpus(\n",
    "    n_train_docs, n_valid_docs, doc_length,\n",
    "    n_topics, n_tokens, alpha\n",
    "):\n",
    "    topic_proba = np.array([1./n_topics] * n_topics)\n",
    "    word_proba = np.random.dirichlet([alpha] * n_tokens, size=n_topics)\n",
    "\n",
    "    corpus, topics = gen_corpus(n_train_docs, n_topics, n_tokens, doc_length, topic_proba, word_proba)\n",
    "    valid_corpus, valid_topics = gen_corpus(n_valid_docs, n_topics, n_tokens, doc_length, topic_proba, word_proba)\n",
    "    \n",
    "    # This is called smoothing : https://en.wikipedia.org/wiki/Smoothing\n",
    "    corpus += 1e-5\n",
    "    valid_corpus += 1e-5\n",
    "    \n",
    "    return topic_proba, word_proba, corpus, valid_corpus\n",
    "\n",
    "# TODO: Implement your method here.\n",
    "# Use the previous function to generate your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application to Text\n",
    "This model is well suited to represent text. In the following, we will apply our EM algorithm to discover topics in the _newsgroup 20_ corpus.\n",
    "\n",
    "As is standard in Natural Language Processing, we will first apply a few pre-processing steps to the corpus as described [here](https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925).\n",
    "\n",
    "Make sure you have installed the following libraries into your conda environment : \n",
    "- gensim\n",
    "- nltk\n",
    "- scikit-learn\n",
    "\n",
    "First, we download the dataset, and wordnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups = {\n",
    "    'train': fetch_20newsgroups(subset='train', shuffle = True),\n",
    "    'test': fetch_20newsgroups(subset='test', shuffle = True)\n",
    "}\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Preprocessing :\n",
    "We use the same pre-processing as in the blog-post :\n",
    "- **Tokenization** : Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "- Words that have fewer than 3 characters are removed.\n",
    "- All **stopwords** are removed.\n",
    "- Words are **lemmatized** - third person is changed to first person, all verbs are changed into present tense\n",
    "- Words are **stemmed** - words are reduced to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result\n",
    "\n",
    "preprocess(\"The quick brown fox jumped over the lazy dogs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary making:\n",
    "Each unique element in the output of _preprocess_ is a token. We need to convert these into unique ids to run our algorithm. This is called building a **dictionary**. The library _gensim_ has a helper function to help us do this.\n",
    "\n",
    "### Question 7:\n",
    "- Why is all this preprocessing useful ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = {s: [] for s in ['train', 'test']}\n",
    "# This may take a while. We are processing the entire dataset.\n",
    "for s in ['train', 'test']:\n",
    "    for doc in newsgroups[s].data:\n",
    "        processed_docs[s].append(preprocess(doc))\n",
    "    \n",
    "dictionary = gensim.corpora.Dictionary(processed_docs['train'])\n",
    "print(f\"{len(dictionary)} unique tokens before filtering\")\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1)\n",
    "print(f\"{len(dictionary)} unique tokens after filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform our corpus of text to lists of counts of tokens\n",
    "bow_corpus = {\n",
    "    s: [dictionary.doc2bow(doc) for doc in processed_docs[s]]\n",
    "    for s in ['train', 'test']\n",
    "}\n",
    "\n",
    "n_tokens = len(dictionary)\n",
    "text_corpus = {'train': None, 'test': None}\n",
    "for s in text_corpus.keys():\n",
    "    n_docs = len(processed_docs[s])\n",
    "    text_corpus[s] = np.zeros((n_docs, n_tokens), dtype='int32')\n",
    "    for d, bow in enumerate(bow_corpus[s]):\n",
    "        for (id, value) in bow:\n",
    "            text_corpus[s][d, id] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model learning\n",
    "Now that the dataset is ready run EM on it, using $5$ topics. Since the dataset is bigger, this could take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New E_step using bag of words rather than dense count arrays\n",
    "def E_step_sparse(corpus, learned_topic_proba, learned_word_proba):\n",
    "    n_topics = learned_topic_proba.shape[0]\n",
    "    res = np.zeros((len(corpus), n_topics))\n",
    "    for d, tuples in enumerate(corpus):\n",
    "        maxi = np.finfo('float64').min\n",
    "        for topic in range(n_topics):\n",
    "            cur = np.log(learned_topic_proba[topic])\n",
    "            for tok_id, v in tuples:\n",
    "                tok_prob = learned_word_proba[topic, tok_id]\n",
    "                cur += np.log(tok_prob) * v\n",
    "            res[d, topic] = cur\n",
    "            maxi = max(maxi, cur)\n",
    "        total = 0\n",
    "        for topic in range(n_topics):\n",
    "            total += np.exp(res[d, topic] - maxi)\n",
    "            res[d, topic] = np.exp(res[d, topic] - maxi)\n",
    "        res[d, :] /= total\n",
    "\n",
    "    return res\n",
    "\n",
    "# New log_likelihood to deal with NaNs\n",
    "def log_likelihood(corpus, learned_word_proba, learned_topic_proba):\n",
    "    \"\"\"\n",
    "    corpus : (S, n_tokens) array containing the counts of words in each document\n",
    "    learned_topic_proba : (n_topics, ) array containing the probability of each topic\n",
    "    learned_word_proba : (n_topics, n_token) array containing the probability of each word for each topic\n",
    "    \"\"\"\n",
    "    all_products = np.log(learned_word_proba[None, :, :]) * corpus[:, None, :]\n",
    "    all_products[np.isnan(all_products)] = 0\n",
    "    A = np.sum(all_products, axis=2) + np.log(learned_topic_proba)[None, :] + log_multinomial(corpus)[:, None]\n",
    "    return np.mean(logsumexp(A))\n",
    "\n",
    "n_iter = 10\n",
    "n_topics = 5\n",
    "n_docs = len(bow_corpus['train'])\n",
    "\n",
    "learned_topic_proba = np.ones((n_topics,)) / n_topics\n",
    "learned_word_proba = np.empty((n_topics, n_tokens), dtype='float32')\n",
    "# initialize topic probabilities with random documents\n",
    "permutation = np.random.permutation(n_docs)\n",
    "batch_size = n_docs // n_topics\n",
    "for t in range(n_topics):\n",
    "    stats = np.sum(text_corpus['train'][permutation[t * batch_size: (t+1) * batch_size]], axis=0)\n",
    "    learned_word_proba[t] = stats / np.sum(stats)\n",
    "\n",
    "train_ll = []\n",
    "valid_ll = []\n",
    "for i in range(n_iter):\n",
    "    # E-step\n",
    "    doc_topic_probas = E_step_sparse(bow_corpus['train'], learned_topic_proba, learned_word_proba)\n",
    "\n",
    "    # E-step for valid\n",
    "    valid_topic_probas = E_step_sparse(bow_corpus['test'], learned_topic_proba, learned_word_proba)\n",
    "\n",
    "    # M-step\n",
    "    learned_topic_proba, learned_word_proba = M_step(text_corpus['train'], doc_topic_probas)\n",
    "\n",
    "    train_ll.append(log_likelihood(text_corpus['train'], learned_word_proba, learned_topic_proba))    \n",
    "    valid_ll.append(log_likelihood(text_corpus['test'], learned_word_proba, learned_topic_proba))\n",
    "    \n",
    "plt.figure(dpi=150)\n",
    "p, = plt.plot(range(len(train_ll)), train_ll, '-', label=f\"{n_topics} topics\")\n",
    "plt.plot(range(len(valid_ll)), valid_ll, '--', c=p.get_color())\n",
    "    \n",
    "plt.xlabel(\"EM-iteration\")\n",
    "plt.ylabel(\"Data log-likelihood\")\n",
    "plt.legend(bbox_to_anchor=(1,1), loc=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the topics\n",
    "\n",
    "Now that we've run EM, we can print the most frequent terms associated to each topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(learned_topic_proba)\n",
    "for t in range(n_topics):\n",
    "    most_frequent = np.argsort(learned_word_proba[t])[-10:]\n",
    "    print(f\"------- Topic {t} ------\")\n",
    "    for w in most_frequent:\n",
    "        print(f\"\\t -- {dictionary[int(w)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "The [perplexity](https://en.wikipedia.org/wiki/Perplexity) of a model $q$ on a test sample $(x_s)$ is given by :\n",
    "$$2^{-\\frac{1}{S}\\sum_{s=1}^S\\log_2(q(x_s))}$$\n",
    "\n",
    "### Question 8 :\n",
    "Explain why this quantity is called perplexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Questions :\n",
    "- In the code implemented for question 6, what happens when we increase the number of documents in the training set ? Comment.\n",
    "- What can you notice regarding the computations done in the E-M algorithm for this practical session ? How would you implement this algorithm in a distributed fashion ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
